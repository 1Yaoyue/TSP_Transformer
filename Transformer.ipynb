{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSP问题优化TSP Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pengji Jin\\AppData\\Local\\Temp\\ipykernel_1148\\2429186550.py:19: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('png2x','pdf')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "#argparse：用于解析命令行参数，方便用户通过命令行传递超参数。\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Categorical：来自 PyTorch 的分布类，用于实现分类变量的概率分布，在强化学习或随机采样中常用。\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# %matplotlib inline：Jupyter Notebook 特有的魔法命令，使绘制的图表直接显示在笔记本中。\n",
    "# set_matplotlib_formats：设置 Matplotlib 输出格式为高分辨率 PNG 和 PDF。\n",
    "# visualization \n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats, clear_output\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "# networkx：用于创建、操作和研究复杂网络结构的图形库，可以辅助 TSP 问题的可视化。\n",
    "# scipy.spatial.distance.pdist 和 squareform：计算城市之间的距离矩阵，对于解决 TSP 问题非常重要。\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置 PyTorch 使用的计算设备（CPU 或 GPU），并且可以指定特定的 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, gpu_id: 0\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cpu\"); gpu_id = -1 # select CPU\n",
    "\n",
    "gpu_id = '0' # select a single GPU  \n",
    "#gpu_id = '2,3' # select multiple GPUs  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))   \n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超参数设置Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb_nodes': 20, 'bsz': 512, 'dim_emb': 128, 'dim_ff': 512, 'dim_input_nodes': 2, 'nb_layers_encoder': 6, 'nb_layers_decoder': 2, 'nb_heads': 8, 'nb_epochs': 10000, 'nb_batch_per_epoch': 2500, 'nb_batch_eval': 20, 'gpu_id': '0', 'lr': 0.0001, 'tol': 0.001, 'batchnorm': True, 'max_len_PE': 1000}\n"
     ]
    }
   ],
   "source": [
    "#定义了一个 DotDict 类，并使用它来创建一个配置对象 args，该对象用于存储训练 TSP（旅行商问题）模型时的各种超参数和设置\n",
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "# nb_nodes：TSP 问题中的城市数量。这里最后被设置为 50，表示解决的是 TSP50 问题。注释掉了其他选项（如 20 和 100），但可以根据需要取消注释以改变问题规模。\n",
    "# bsz：批次大小，即每个训练步骤处理的样本数量。这里设置为 512。\n",
    "# dim_emb：嵌入维度，通常用于表示节点特征的空间维度。\n",
    "# dim_ff：前馈网络（Feed-forward Network）的维度，常用于 Transformer 模型中的中间层。\n",
    "# dim_input_nodes：每个节点输入特征的数量。对于 TSP 问题，通常是二维坐标 (x, y)，所以设置为 2。\n",
    "# nb_layers_encoder 和 nb_layers_decoder：编码器和解码器的层数，分别设置为 6 和 2。\n",
    "# nb_heads：多头注意力机制中的头数，设置为 8。\n",
    "# nb_epochs：训练的总周期数，设置为 10000。\n",
    "# nb_batch_per_epoch：每个 epoch 中的批次数量，设置为 2500。\n",
    "# nb_batch_eval：评估过程中使用的批次数量，设置为 20。\n",
    "# gpu_id：GPU ID，从之前设置的变量 gpu_id 获取。\n",
    "# lr：学习率，设置为 1e-4。\n",
    "# tol：容差值，用于基线更新条件，设置为 1e-3。\n",
    "# batchnorm：是否使用批量归一化（Batch Normalization）。这里设置为 True，意味着使用批量归一化；注释掉的选项是使用层归一化（Layer Normalization）。\n",
    "# max_len_PE：位置编码的最大长度，设置为 1000。   \n",
    "# 1. 节点数量 (nb_nodes)\n",
    "# 当前设置：args.nb_nodes = 50\n",
    "# 建议：对于不同的TSP问题规模（如TSP20, TSP50, TSP100），确保你的模型架构和超参数能够适应不同规模的问题。如果你打算处理更大的实例（例如TSP100），可能需要增加模型容量或调整其他参数。\n",
    "# 2. 批次大小 (bsz)\n",
    "# 当前设置：args.bsz = 512\n",
    "# 建议：较大的批次大小可以加速训练并利用GPU的优势，但也可能导致内存不足。尝试找到一个平衡点，既能充分利用硬件资源，又不会超出显存限制。如果遇到内存问题，考虑减小批次大小。\n",
    "# 3. 嵌入维度 (dim_emb) 和前馈网络维度 (dim_ff)\n",
    "# 当前设置：args.dim_emb = 128, args.dim_ff = 512\n",
    "# 建议：这些值看起来是合理的，但对于更复杂的任务或更大规模的数据集，可以尝试增加这两个维度以增强模型表达能力。同时要注意，增大这些值会显著增加计算量和内存需求。\n",
    "# 4. 输入维度 (dim_input_nodes)\n",
    "# 当前设置：args.dim_input_nodes = 2\n",
    "# 建议：保持不变，因为这是由TSP问题本身决定的（每个节点有二维坐标）。\n",
    "# 5. 编码器层数 (nb_layers_encoder) 和解码器层数 (nb_layers_decoder)\n",
    "# 当前设置：args.nb_layers_encoder = 6, args.nb_layers_decoder = 2\n",
    "# 建议：编码器层数通常是解码器层数的两倍或更多，这有助于捕捉输入序列中的复杂模式。你可以根据实验结果调整这两者之间的比例。如果发现模型过拟合，可以尝试减少层数；如果欠拟合，则可以适当增加。\n",
    "# 6. 注意力头数 (nb_heads)\n",
    "# 当前设置：args.nb_heads = 8\n",
    "# 建议：这个值通常是合理的，但也可以尝试不同的头数来观察效果。更多的头数可以让模型学习到更细粒度的信息，但也增加了计算负担。\n",
    "# 7. 训练轮数 (nb_epochs) 和每轮批次数 (nb_batch_per_epoch)\n",
    "# 当前设置：args.nb_epochs = 10000, args.nb_batch_per_epoch = 2500\n",
    "# 建议：如此多的训练轮数可能没有必要，特别是如果你使用早停（early stopping）策略。监控验证集上的性能，并在性能不再提升时停止训练。每轮批次数可以根据数据集大小和可用计算资源进行调整。\n",
    "# 8. 评估批次数 (nb_batch_eval)\n",
    "# 当前设置：args.nb_batch_eval = 20\n",
    "# 建议：保持合理数量的评估批次，以获得对模型性能的稳定估计。如果评估集较大，可以适当增加这个值。\n",
    "# 9. 学习率 (lr)\n",
    "# 当前设置：args.lr = 1e-4\n",
    "# 建议：学习率的选择非常关键。可以从较低的学习率开始，然后通过学习率调度器（如线性衰减、余弦退火等）逐步调整。此外，使用学习率查找器（learning rate finder）可以帮助确定最优初始学习率。\n",
    "# 10. 容忍度 (tol)\n",
    "# 当前设置：args.tol = 1e-3\n",
    "# 建议：这个参数用于判断是否停止训练，具体取决于你使用的优化算法。如果使用早停机制，确保容忍度设置合理，既不过于宽松也不过于严格。\n",
    "# 11. 归一化 (batchnorm vs layernorm)\n",
    "# 当前设置：args.batchnorm = True\n",
    "# 建议：Batch Normalization（BN）适用于批处理大小较大的情况，而 Layer Normalization（LN）则更适合较小的批处理大小或长序列。考虑到Transformer模型的特点，Layer Norm通常表现更好。可以尝试切换到Layer Norm，看看是否能带来性能提升。\n",
    "# 12. 位置编码最大长度 (max_len_PE)\n",
    "# 当前设置：args.max_len_PE = 1000\n",
    "# 建议：这个值应该足够大以覆盖所有可能的序列长度。如果你确实不需要这么大的值，可以适当减小以节省内存。     \n",
    "args = DotDict()\n",
    "args.nb_nodes = 20 # TSP20\n",
    "# args.nb_nodes = 50 # TSP50\n",
    "# args.nb_nodes = 100 # TSP100\n",
    "args.bsz = 512 # TSP20 TSP50\n",
    "args.dim_emb = 128\n",
    "args.dim_ff = 512\n",
    "args.dim_input_nodes = 2\n",
    "args.nb_layers_encoder = 6\n",
    "args.nb_layers_decoder = 2\n",
    "args.nb_heads = 8\n",
    "args.nb_epochs = 10000\n",
    "args.nb_batch_per_epoch = 2500\n",
    "args.nb_batch_eval = 20\n",
    "args.gpu_id = gpu_id\n",
    "args.lr = 1e-4\n",
    "args.tol = 1e-3\n",
    "args.batchnorm = True  # if batchnorm=True  than batch norm is used\n",
    "#args.batchnorm = False # if batchnorm=False than layer norm is used\n",
    "args.max_len_PE = 1000\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成并保存测试集generate and save test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20, 2]) tensor([[0.3670, 0.7802],\n",
      "        [0.2989, 0.9602],\n",
      "        [0.4519, 0.8380],\n",
      "        [0.6201, 0.9643],\n",
      "        [0.5885, 0.6127],\n",
      "        [0.2668, 0.0044],\n",
      "        [0.8015, 0.2750],\n",
      "        [0.7790, 0.3143],\n",
      "        [0.5775, 0.4547],\n",
      "        [0.1175, 0.7685],\n",
      "        [0.1504, 0.3194],\n",
      "        [0.7096, 0.9206],\n",
      "        [0.2231, 0.9546],\n",
      "        [0.4840, 0.7846],\n",
      "        [0.3182, 0.7066],\n",
      "        [0.0043, 0.5257],\n",
      "        [0.1106, 0.8115],\n",
      "        [0.3645, 0.5307],\n",
      "        [0.9666, 0.0883],\n",
      "        [0.5323, 0.7243]])\n",
      "nb of nodes : 20\n"
     ]
    }
   ],
   "source": [
    "# save_1000tsp：布尔变量，决定是否生成并保存新的测试集。这里被设置为 False，意味着不会执行保存操作。\n",
    "# 生成测试集：\n",
    "# 如果 save_1000tsp 为 True，则会生成包含 1000 个实例的测试集，每个实例有 args.nb_nodes 个城市，每个城市有两个坐标值（dim_input_nodes=2）。\n",
    "# 根据 nb_node 决定生成测试集的名字\n",
    "# 数据在 CPU 上生成，并打印其大小和第一个实例的内容以供检查。\n",
    "# 保存测试集：\n",
    "# 创建 data 目录（如果不存在）。\n",
    "# 根据 args.nb_nodes 的值将测试集保存为对应的文件名（例如 1000tsp20.pkl），以便后续使用。\n",
    "save_1000tsp = True\n",
    "# save_1000tsp = False\n",
    "if save_1000tsp:\n",
    "    bsz = 1000\n",
    "    x = torch.rand(bsz, args.nb_nodes, args.dim_input_nodes, device='cpu') \n",
    "    print(x.size(),x[0])\n",
    "    data_dir = os.path.join(\"data\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if args.nb_nodes==20 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp20\"))\n",
    "    if args.nb_nodes==50 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp50\"))\n",
    "    if args.nb_nodes==100 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp100\"))\n",
    "\n",
    "# 尝试加载现有测试集：\n",
    "# 根据 args.nb_nodes 的值尝试从 data 目录加载相应的测试集文件。\n",
    "# 如果成功加载，则将数据移动到当前设备（CPU 或 GPU），并记录城市数量 n。\n",
    "# 如果没有找到已保存的测试集：\n",
    "# 在 CPU 上重新生成 1000 个实例的测试集。\n",
    "# 记录城市数量 n。\n",
    "checkpoint = None\n",
    "if args.nb_nodes==20 : checkpoint = torch.load(\"data/1000tsp20.pkl\")\n",
    "if args.nb_nodes==50 : checkpoint = torch.load(\"data/1000tsp50.pkl\")\n",
    "if args.nb_nodes==100 : checkpoint = torch.load(\"data/1000tsp100.pkl\")\n",
    "if checkpoint is not None:\n",
    "    x_1000tsp = checkpoint['x'].to(device)\n",
    "    n = x_1000tsp.size(1)\n",
    "    print('nb of nodes :',n)\n",
    "else:\n",
    "    x_1000tsp = torch.rand(1000, args.nb_nodes, args.dim_input_nodes, device='cpu')\n",
    "    n = x_1000tsp.size(1)\n",
    "    print('nb of nodes :',n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "路径长度计算compute tour length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tour_length(x, tour): \n",
    "    \"\"\"\n",
    "    Compute the length of a batch of tours\n",
    "    Inputs : x of size (bsz, nb_nodes, 2) batch of tsp tour instances\n",
    "             tour of size (bsz, nb_nodes) batch of sequences (node indices) of tsp tours\n",
    "    Output : L of size (bsz,)             batch of lengths of each tsp tour\n",
    "    \"\"\"\n",
    "    bsz = x.shape[0]\n",
    "    nb_nodes = x.shape[1]#这里从输入张量 x 中提取了批量大小 bsz 和每个实例中的节点数量 nb_nodes。\n",
    "    arange_vec = torch.arange(bsz, device=x.device)#创建了一个范围向量 arange_vec，用于索引批量中的每一个样本。这有助于在后续代码中高效地选择特定于每个样本的城市坐标。\n",
    "    first_cities = x[arange_vec, tour[:,0], :] # size(first_cities)=(bsz,2)\n",
    "    previous_cities = first_cities#选择了每个路径的第一个城市，并将这些城市设为 previous_cities，以便开始计算路径长度。\n",
    "    L = torch.zeros(bsz, device=x.device)#初始化一个形状为 (bsz,) 的零张量 L，用来累积每条路径的长度。\n",
    "    with torch.no_grad():\n",
    "        for i in range(1,nb_nodes):\n",
    "            current_cities = x[arange_vec, tour[:,i], :] \n",
    "            L += torch.sum( (current_cities - previous_cities)**2 , dim=1 )**0.5 # dist(current, previous node) \n",
    "            previous_cities = current_cities\n",
    "        L += torch.sum( (current_cities - first_cities)**2 , dim=1 )**0.5 # dist(last, first node)  \n",
    "    return L#遍历每个路径上的所有城市对（除了最后一个到第一个城市之间的距离）。对于每一对连续的城市，它计算欧几里得距离并将结果加到 L 中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络部分Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码器encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module 是 PyTorch 中所有神经网络模型的基础类。如果你想要创建一个自定义的神经网络，你需要继承 nn.Module 并实现至少一个方法：forward()\n",
    "class Transformer_encoder_net(nn.Module):\n",
    "    def __init__(self, nb_layers, dim_emb, nb_heads, dim_ff, batchnorm):\n",
    "        super(Transformer_encoder_net, self).__init__()\n",
    "        # 确认嵌入维度 dim_emb 可以被头数 nb_heads 整除\n",
    "        # 这是因为每个头将处理一部分嵌入维度,输入的嵌入向量会被分割成多个较小的向量，每个向量对应一个“头”。如果 dim_emb 能够被 nb_heads 整除，那么每个头可以均匀地分配到相同大小的子空间进行并行计算。\n",
    "        assert dim_emb == nb_heads* (dim_emb//nb_heads) \n",
    "        # 创建一个包含 nb_layers 个 MultiheadAttention 层的列表。\n",
    "        # 每个注意力层都接收相同大小的输入，并输出相同大小的结果。\n",
    "        self.MHA_layers = nn.ModuleList( [nn.MultiheadAttention(dim_emb, nb_heads) for _ in range(nb_layers)] )\n",
    "\n",
    "        #为每个编码器层创建两个线性变换层：第一个将输入从 dim_emb 映射到更大的维度 dim_ff，第二个再映射回 dim_emb。这些用于实现前馈神经网络部分。\n",
    "        #nn.Linear和以下的两个都是Pytorch中的模块\n",
    "        self.linear1_layers = nn.ModuleList( [nn.Linear(dim_emb, dim_ff) for _ in range(nb_layers)] )\n",
    "        self.linear2_layers = nn.ModuleList( [nn.Linear(dim_ff, dim_emb) for _ in range(nb_layers)] )   \n",
    "        #根据是否启用了批量归一化，创建相应的归一化层列表。如果未启用，则使用层归一化。\n",
    "        if batchnorm:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n",
    "        else:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n",
    "        #保存参数名\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_heads = nb_heads\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "    def forward(self, h):\n",
    "        # PyTorch nn.MultiheadAttention requires input size (seq_len, bsz, dim_emb) \n",
    "        # size(h)=(nb_nodes, bsz, dim_emb)  形状为 (bsz, nb_nodes+1, dim_emb) 的张量，表示一批大小为 bsz 的节点（城市）嵌入，每个节点有 dim_emb 维的特征。\n",
    "        # score为注意力分数，表示节点间的关系，h为输入数据经过多层编码器处理后得到的更新节点嵌入\n",
    "        # 将张量的0,1项交换\n",
    "        h = h.transpose(0,1) \n",
    "        # L layers接下来是遍历每一层编码器的循环：\n",
    "        for i in range(self.nb_layers):\n",
    "            #多头注意力机制 (MHA)\n",
    "            h_rc = h # residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)残差连接 (residual connection)：在应用多头注意力之前保存当前状态 h_rc，以便之后可以添加回输出中。\n",
    "            #调用多头注意力层（已在前面定义） self.MHA_layers[i]，传入相同的查询、键和值（即 h），得到更新后的 h 和注意力分数 score。\n",
    "            h, score = self.MHA_layers[i](h, h, h) # size(h)=(nb_nodes, bsz, dim_emb), size(score)=(bsz, nb_nodes, nb_nodes)\n",
    "            # add residual connection\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)将原始输入 h_rc 添加到多头注意力的输出 h 上，实现跳跃连接，有助于缓解深度网络中的梯度消失问题。\n",
    "            #根据是否启用了批量归一化（Batch Normalization），选择不同的归一化方式\n",
    "            #如果使用批量归一化，则需要调整张量形状以匹配 BatchNorm1d 的期望输入格式 (bsz, dim, seq_len)，应用归一化后再恢复原形状。\n",
    "            #否则直接应用层归一化（Layer Normalization），保持原有形状不变。\n",
    "            #归一化\n",
    "            if self.batchnorm:\n",
    "                # Pytorch nn.BatchNorm1d requires input size (bsz, dim, seq_len)\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(nb_nodes, bsz, dim_emb) \n",
    "            # feedforward前馈神经网络 (FFN)\n",
    "            # 再次使用残差连接保存当前状态 h_rc。\n",
    "            # 应用ReLU激活函数后的线性变换，然后再次进行线性变换以返回原始嵌入维度。\n",
    "            # 添加残差连接，确保信息流动的同时引入非线性。\n",
    "            h_rc = h # residual connection\n",
    "            h = self.linear2_layers[i](torch.relu(self.linear1_layers[i](h)))\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            #第二次归一化\n",
    "            if self.batchnorm:\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm2_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm2_layers[i](h) # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "        # Transpose h在所有编码器层处理完毕后，将张量重新排列回初始形状 (bsz, nb_nodes, dim_emb)，以便后续处理或输出。\n",
    "        h = h.transpose(0,1) # size(h)=(bsz, nb_nodes, dim_emb)\n",
    "        return h, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义的多头注意力（Multi-Head Attention, MHA）机制，旨在避免在每次调用时重新计算所有的线性变换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入\n",
    "# Q: 形状为 (bsz, dim_emb, 1) 的张量，表示一批查询向量。\n",
    "# K: 形状为 (bsz, dim_emb, nb_nodes+1) 的张量，表示一批键向量。\n",
    "# V: 形状为 (bsz, dim_emb, nb_nodes+1) 的张量，表示一批值向量。\n",
    "# nb_heads: 多头注意力中的头数。\n",
    "# mask (可选): 形状为 (bsz, nb_nodes+1) 的张量，用于遮蔽某些节点，例如已经访问过的城市。\n",
    "# clip_value (可选): 一个标量，用于剪切注意力权重。\n",
    "# 输出\n",
    "# attn_output: 形状为 (bsz, 1, dim_emb) 的张量，表示一批注意力输出向量。\n",
    "# attn_weights: 形状为 (bsz, 1, nb_nodes+1) 的张量，表示一批注意力权重。\n",
    "def myMHA(Q, K, V, nb_heads, mask=None, clip_value=None):\n",
    "    \"\"\"\n",
    "    Compute multi-head attention (MHA) given a query Q, key K, value V and attention mask :\n",
    "      h = Concat_{k=1}^nb_heads softmax(Q_k^T.K_k).V_k \n",
    "    Note : We did not use nn.MultiheadAttention to avoid re-computing all linear transformations at each call.\n",
    "    Inputs : Q of size (bsz, dim_emb, 1)                batch of queries\n",
    "             K of size (bsz, dim_emb, nb_nodes+1)       batch of keys\n",
    "             V of size (bsz, dim_emb, nb_nodes+1)       batch of values\n",
    "             mask of size (bsz, nb_nodes+1)             batch of masks of visited cities\n",
    "             clip_value is a scalar \n",
    "    Outputs : attn_output of size (bsz, 1, dim_emb)     batch of attention vectors\n",
    "              attn_weights of size (bsz, 1, nb_nodes+1) batch of attention weights\n",
    "    \"\"\"\n",
    "    # 这里从 K 中提取了批量大小 bsz、节点数量 nb_nodes 和嵌入维度 emd_dim。同时，注释中提到 dim_emb 必须可以被 nb_heads 整除，以确保每个头处理相同大小的子空间。\n",
    "    bsz, nb_nodes, emd_dim = K.size() #  dim_emb must be divisable by nb_heads\n",
    "    # 当 nb_heads > 1 ,即有多个头时，需要将 Q、K 和 V 张量重新排列并分割成多个头,确保了每个头能够独立地处理不同部分的嵌入向量，同时也保证了形状的一致性。\n",
    "    if nb_heads>1:\n",
    "        # PyTorch view requires contiguous dimensions for correct reshaping\n",
    "        Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz, dim_emb, 1)\n",
    "        Q = Q.view(bsz*nb_heads, emd_dim//nb_heads, 1) # size(Q)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n",
    "        Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n",
    "        K = K.transpose(1,2).contiguous() # size(K)=(bsz, dim_emb, nb_nodes+1)\n",
    "        K = K.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(K)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n",
    "        K = K.transpose(1,2).contiguous() # size(K)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n",
    "        V = V.transpose(1,2).contiguous() # size(V)=(bsz, dim_emb, nb_nodes+1)\n",
    "        V = V.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(V)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n",
    "        V = V.transpose(1,2).contiguous() # size(V)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n",
    "    #批矩阵乘法 (torch.bmm) 来计算查询与键之间的点积，然后除以根号下的查询维度，这一步是为了缩放点积结果，防止梯度消失或爆炸问题。\n",
    "    attn_weights = torch.bmm(Q, K.transpose(1,2))/ Q.size(-1)**0.5 # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    #剪切：通过 tanh 函数限制注意力权重的范围，并乘以 clip_value。\n",
    "    #遮蔽：对于已经被访问过的节点，应用极大的负值（如 -1e9），使得它们在 softmax 后的概率接近于零。\n",
    "    if clip_value is not None:\n",
    "        attn_weights = clip_value * torch.tanh(attn_weights)\n",
    "    if mask is not None:\n",
    "        if nb_heads>1:\n",
    "            mask = torch.repeat_interleave(mask, repeats=nb_heads, dim=0) # size(mask)=(bsz*nb_heads, nb_nodes+1)\n",
    "        #attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), float('-inf')) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "        attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), float('-1e9')) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    attn_weights = torch.softmax(attn_weights, dim=-1) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)应用 softmax 函数，将注意力权重转换为概率分布。\n",
    "    attn_output = torch.bmm(attn_weights, V) # size(attn_output)=(bsz*nb_heads, 1, dim_emb//nb_heads)再次使用批矩阵乘法来计算加权和，得到最终的注意力输出。\n",
    "    if nb_heads>1:\n",
    "        attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n",
    "        attn_output = attn_output.view(bsz, emd_dim, 1) # size(attn_output)=(bsz, dim_emb, 1)\n",
    "        attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz, 1, dim_emb)\n",
    "        attn_weights = attn_weights.view(bsz, nb_heads, 1, nb_nodes) # size(attn_weights)=(bsz, nb_heads, 1, nb_nodes+1)\n",
    "        attn_weights = attn_weights.mean(dim=1) # mean over the heads, size(attn_weights)=(bsz, 1, nb_nodes+1)\n",
    "    return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自回归解码器层，主要用于生成序列数据的任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single decoder layer based on self-attention and query-attention\n",
    "    Inputs :  \n",
    "      h_t of size      (bsz, 1, dim_emb)          batch of input queries\n",
    "      K_att of size    (bsz, nb_nodes+1, dim_emb) batch of query-attention keys\n",
    "      V_att of size    (bsz, nb_nodes+1, dim_emb) batch of query-attention values\n",
    "      mask of size     (bsz, nb_nodes+1)          batch of masks of visited cities\n",
    "    Output :  \n",
    "      h_t of size (bsz, nb_nodes+1)               batch of transformed queries\n",
    "    \"\"\"\n",
    "    #\n",
    "    def __init__(self, dim_emb, nb_heads):\n",
    "        super(AutoRegressiveDecoderLayer, self).__init__()\n",
    "        self.dim_emb = dim_emb# 嵌入维度\n",
    "        self.nb_heads = nb_heads# 注意力头的数量\n",
    "\n",
    "         # 自注意力机制中的线性变换层\n",
    "        self.Wq_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wk_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wv_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W0_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "        # 查询注意力机制中的线性变换层\n",
    "        self.W0_att = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wq_att = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W1_MLP = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W2_MLP = nn.Linear(dim_emb, dim_emb)\n",
    "\n",
    "         # 批规范化层\n",
    "        self.BN_selfatt = nn.LayerNorm(dim_emb)\n",
    "        self.BN_att = nn.LayerNorm(dim_emb)\n",
    "        self.BN_MLP = nn.LayerNorm(dim_emb)\n",
    "        \n",
    "         # 初始化累积的自注意力键和值为 None\n",
    "        self.K_sa = None\n",
    "        self.V_sa = None\n",
    "\n",
    "    #将 self.K_sa 和 self.V_sa 设置为 None，用于清除之前的累积键和值，这通常是在处理新序列时调用的。\n",
    "    def reset_selfatt_keys_values(self):\n",
    "        self.K_sa = None\n",
    "        self.V_sa = None\n",
    "        \n",
    "\n",
    "    #h_t扮演query的角色，前向传播函数\n",
    "    def forward(self, h_t, K_att, V_att, mask):\n",
    "        bsz = h_t.size(0) # 获取批次大小\n",
    "        h_t = h_t.view(bsz,1,self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)调整 h_t 的形状以适应后续操作\n",
    "        # embed the query for self-attention 自注意力机制\n",
    "        q_sa = self.Wq_selfatt(h_t) # size(q_sa)=(bsz, 1, dim_emb)\n",
    "        k_sa = self.Wk_selfatt(h_t) # size(k_sa)=(bsz, 1, dim_emb)\n",
    "        v_sa = self.Wv_selfatt(h_t) # size(v_sa)=(bsz, 1, dim_emb)\n",
    "        # concatenate the new self-attention key and value to the previous keys and values# 累积自注意力键和值\n",
    "        #检查是否为第一个时间步，是的话初始化累积的键和值，不是的话使用 torch.cat 函数沿指定维度（这里是 dim=1）拼接现有的累积键/值与当前时间步的键/值。\n",
    "        if self.K_sa is None:\n",
    "            self.K_sa = k_sa # size(self.K_sa)=(bsz, 1, dim_emb)\n",
    "            self.V_sa = v_sa # size(self.V_sa)=(bsz, 1, dim_emb)\n",
    "        else:\n",
    "            self.K_sa = torch.cat([self.K_sa, k_sa], dim=1)\n",
    "            self.V_sa = torch.cat([self.V_sa, v_sa], dim=1)\n",
    "\n",
    "        # compute self-attention between nodes in the partial tour  计算自注意力并更新 h_t\n",
    "        h_t = h_t + self.W0_selfatt( myMHA(q_sa, self.K_sa, self.V_sa, self.nb_heads)[0] ) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        h_t = self.BN_selfatt(h_t.squeeze()) # size(h_t)=(bsz, dim_emb)# 应用批规范化\n",
    "        h_t = h_t.view(bsz, 1, self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)# 调整形状\n",
    "        # compute attention between self-attention nodes and encoding nodes in the partial tour (translation process)\n",
    "        ## 查询注意力机制\n",
    "        q_a = self.Wq_att(h_t) # size(q_a)=(bsz, 1, dim_emb)\n",
    "        h_t = h_t + self.W0_att( myMHA(q_a, K_att, V_att, self.nb_heads, mask)[0] ) # size(h_t)=(bsz, 1, dim_emb)更新 h_t\n",
    "        h_t = self.BN_att(h_t.squeeze()) # size(h_t)=(bsz, dim_emb)\n",
    "        h_t = h_t.view(bsz, 1, self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # MLP # MLP 层\n",
    "        h_t = h_t + self.W2_MLP(torch.relu(self.W1_MLP(h_t)))\n",
    "        h_t = self.BN_MLP(h_t.squeeze(1)) # size(h_t)=(bsz, dim_emb)\n",
    "        return h_t\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码器层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_decoder_net(nn.Module): \n",
    "    \"\"\"\n",
    "    Decoder network based on self-attention and query-attention transformers\n",
    "    Inputs :  \n",
    "      h_t of size      (bsz, 1, dim_emb)                            batch of input queries\n",
    "      K_att of size    (bsz, nb_nodes+1, dim_emb*nb_layers_decoder) batch of query-attention keys for all decoding layers\n",
    "      V_att of size    (bsz, nb_nodes+1, dim_emb*nb_layers_decoder) batch of query-attention values for all decoding layers\n",
    "      mask of size     (bsz, nb_nodes+1)                            batch of masks of visited cities\n",
    "    Output :  \n",
    "      prob_next_node of size (bsz, nb_nodes+1)                      batch of probabilities of next node\n",
    "    \"\"\"\n",
    "    #类初始化 (__init__)，self是类的第一个参数，后面三个是类的实例属性，也就是主要看后三个\n",
    "    def __init__(self, dim_emb, nb_heads, nb_layers_decoder):\n",
    "        super(Transformer_decoder_net, self).__init__()\n",
    "        self.dim_emb = dim_emb # 嵌入维度，每个时间步输入和输出的特征维度\n",
    "        self.nb_heads = nb_heads # 注意力头的数量\n",
    "        self.nb_layers_decoder = nb_layers_decoder # 解码器层数\n",
    "        # AutoRegressiveDecoderLayer 层是 Transformer_decoder_net 中的一个关键组件，\n",
    "        # 它实现了基于自注意力（self-attention）和查询注意力（query-attention）机制的解码器层。\n",
    "        # 这种类型的解码器层特别适用于自动回归（auto-regressive）模型，其中每个时间步的输出依赖于之前所有时间步的输出。\n",
    "        self.decoder_layers = nn.ModuleList( [AutoRegressiveDecoderLayer(dim_emb, nb_heads) for _ in range(nb_layers_decoder-1)] )# 创建多个 AutoRegressiveDecoderLayer 层\n",
    "        # 最终一层的线性变换层\n",
    "        self.Wq_final = nn.Linear(dim_emb, dim_emb)\n",
    "        \n",
    "    # Reset to None self-attention keys and values when decoding starts \n",
    "    #重置累积的自注意力键和值 (reset_selfatt_keys_values)，通常在开始处理新序列时调用\n",
    "    def reset_selfatt_keys_values(self): \n",
    "        for l in range(self.nb_layers_decoder-1):\n",
    "            self.decoder_layers[l].reset_selfatt_keys_values()\n",
    "            \n",
    "    #前向传播 (forward)\n",
    "    def forward(self, h_t, K_att, V_att, mask):\n",
    "        for l in range(self.nb_layers_decoder):\n",
    "            K_att_l = K_att[:,:,l*self.dim_emb:(l+1)*self.dim_emb].contiguous()  # size(K_att_l)=(bsz, nb_nodes+1, dim_emb) # 提取当前层对应的K_att部分\n",
    "            V_att_l = V_att[:,:,l*self.dim_emb:(l+1)*self.dim_emb].contiguous()  # size(V_att_l)=(bsz, nb_nodes+1, dim_emb) # 提取当前层对应的V_att部分\n",
    "            if l<self.nb_layers_decoder-1: # decoder layers with multiple heads (intermediate layers) # 对于中间层，不是最后一层，则调用相应的 AutoRegressiveDecoderLayer 进行处理，并更新 h_t\n",
    "                h_t = self.decoder_layers[l](h_t, K_att_l, V_att_l, mask)\n",
    "            else: # decoder layers with single head (final layer)# 对于最后一层，使用 Wq_final 线性变换来生成最终的查询 q_final，然后通过 myMHA 函数计算注意力权重 attn_weights，这里使用单头注意力机制。\n",
    "                q_final = self.Wq_final(h_t)\n",
    "                bsz = h_t.size(0)\n",
    "                q_final = q_final.view(bsz, 1, self.dim_emb)\n",
    "                attn_weights = myMHA(q_final, K_att_l, V_att_l, 1, mask, 10)[1] \n",
    "        prob_next_node = attn_weights.squeeze(1) #attn_weights 被挤压成形状 (bsz, nb_nodes+1) 的张量，表示下一个节点的概率分布 prob_next_node。\n",
    "        return prob_next_node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "位置编码,新论文删除了来轻量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成标准的 Transformer 位置编码，详见transformer的ppt\n",
    "def generate_positional_encoding(d_model, max_len):\n",
    "    \"\"\"\n",
    "    Create standard transformer PEs.\n",
    "    Inputs :  \n",
    "      d_model is a scalar correspoding to the hidden dimension\n",
    "      max_len is the maximum length of the sequence\n",
    "    Output :  \n",
    "      pe of size (max_len, d_model), where d_model=dim_emb, max_len=1000\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model) # 初始化位置编码矩阵   \n",
    "    # 计算位置索引，使用 torch.arange 创建一个从 0 到 max_len-1 的浮点数张量，并通过 unsqueeze(1) 将其转换为二维张量，使得它可以与后续的 div_term 进行广播相乘。\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) \n",
    "    # 计算除法因子 (div_term)，公式\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    #对于所有偶数维度（0::2），使用正弦函数填充位置编码。\n",
    "    #对于所有奇数维度（1::2），使用余弦函数填充位置编码。\n",
    "    pe[:,0::2] = torch.sin(position * div_term)\n",
    "    pe[:,1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合TSP问题，结合了编码器-解码器架构和自回归解码过程In the context of the TSP problem, it integrates an encoder-decoder architecture with an autoregressive decoding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSP_net(nn.Module): \n",
    "    \"\"\"\n",
    "    The TSP network is composed of two steps :\n",
    "      Step 1. Encoder step : Take a set of 2D points representing a fully connected graph \n",
    "                             and encode the set with self-transformer.\n",
    "      Step 2. Decoder step : Build the TSP tour recursively/autoregressively, \n",
    "                             i.e. one node at a time, with a self-transformer and query-transformer. \n",
    "    Inputs : \n",
    "      x of size (bsz, nb_nodes, dim_emb) Euclidian coordinates of the nodes/cities\n",
    "      deterministic is a boolean : If True the salesman will chose the city with highest probability. \n",
    "                                   If False the salesman will chose the city with Bernouilli sampling.\n",
    "    Outputs : \n",
    "      tours of size (bsz, nb_nodes) : batch of tours, i.e. sequences of ordered cities \n",
    "                                      tours[b,t] contains the idx of the city visited at step t in batch b\n",
    "      sumLogProbOfActions of size (bsz,) : batch of sum_t log prob( pi_t | pi_(t-1),...,pi_0 )\n",
    "    \"\"\"\n",
    "    # dim_input_nodes: 输入节点的维度，例如每个城市坐标的维度。\n",
    "    # dim_emb: 嵌入维度，所有内部表示的统一维度。\n",
    "    # dim_ff: 前馈神经网络中的隐藏层维度。\n",
    "    # nb_layers_encoder: 编码器中 Transformer 层的数量。\n",
    "    # nb_layers_decoder: 解码器中 Transformer 层的数量。\n",
    "    # nb_heads: 注意力机制中的头数量。\n",
    "    # max_len_PE: 位置编码的最大长度。\n",
    "    # batchnorm: 是否使用批规范化，默认为 True。\n",
    "    def __init__(self, dim_input_nodes, dim_emb, dim_ff, nb_layers_encoder, nb_layers_decoder, nb_heads, max_len_PE,\n",
    "                 batchnorm=True):\n",
    "        super(TSP_net, self).__init__()\n",
    "        \n",
    "        self.dim_emb = dim_emb\n",
    "        \n",
    "        # input embedding layer\n",
    "        self.input_emb = nn.Linear(dim_input_nodes, dim_emb)\n",
    "        \n",
    "        # encoder layer从编码层输入\n",
    "        self.encoder = Transformer_encoder_net(nb_layers_encoder, dim_emb, nb_heads, dim_ff, batchnorm)\n",
    "        \n",
    "        # vector to start decoding 定义一个可训练的参数，作为解码开始时的占位符，形状为 (dim_emb,)\n",
    "        self.start_placeholder = nn.Parameter(torch.randn(dim_emb))\n",
    "        \n",
    "        # decoder layer\n",
    "        self.decoder = Transformer_decoder_net(dim_emb, nb_heads, nb_layers_decoder)\n",
    "        self.WK_att_decoder = nn.Linear(dim_emb, nb_layers_decoder* dim_emb) \n",
    "        self.WV_att_decoder = nn.Linear(dim_emb, nb_layers_decoder* dim_emb) \n",
    "        self.PE = generate_positional_encoding(dim_emb, max_len_PE)        \n",
    "        \n",
    "    #前向传播\n",
    "    # 输入：\n",
    "    # x: 形状为 (bsz, nb_nodes, dim_emb) 的张量，表示一批城市的欧几里得坐标。\n",
    "    # deterministic: 决定选择下一个城市的方式——确定性（最高概率）或随机采样（伯努利抽样）。\n",
    "    # 输出：\n",
    "    # tours: 形状为 (bsz, nb_nodes) 的张量，表示一批访问顺序的城市索引。\n",
    "    # sumLogProbOfActions: 形状为 (bsz,) 的张量，表示每个样本的动作序列的累积对数概率。\n",
    "    def forward(self, x, deterministic=False):\n",
    "\n",
    "        # some parameters获取批量大小 bsz 和节点数量 nb_nodes。\n",
    "        #创建从 0 到 bsz-1 的索引数组 zero_to_bsz，用于后续操作。\n",
    "        bsz = x.shape[0]\n",
    "        nb_nodes = x.shape[1]\n",
    "        zero_to_bsz = torch.arange(bsz, device=x.device) # [0,1,...,bsz-1]\n",
    "\n",
    "        # input embedding layer 用self.input_emb 对输入 x 进行线性变换，得到形状为 (bsz, nb_nodes, dim_emb) 的嵌入表示 h。\n",
    "        h = self.input_emb(x) # size(h)=(bsz, nb_nodes, dim_emb)\n",
    "        \n",
    "        # concat the nodes and the input placeholder that starts the decoding\n",
    "        # 将解码起始占位符与 h 拼接，形成形状为 (bsz, nb_nodes+1, dim_emb) 的新张量。这允许解码器知道序列的起点。\n",
    "        h = torch.cat([h, self.start_placeholder.repeat(bsz, 1, 1)], dim=1) # size(start_placeholder)=(bsz, nb_nodes+1, dim_emb)\n",
    "        \n",
    "        # encoder layer\n",
    "        #通过 self.encoder 对拼接后的张量 h 进行编码，得到编码后的表示 h_encoder。编码器会根据上下文信息更新每个节点的表示。\n",
    "        h_encoder, _ = self.encoder(h) # size(h)=(bsz, nb_nodes+1, dim_emb)\n",
    "\n",
    "        # list that will contain Long tensors of shape (bsz,) that gives the idx of the cities chosen at time t\n",
    "        # 这是一个空列表，用于保存一系列形状为 (bsz,) 的长整型张量（LongTensor），其中 bsz 表示批次大小。\n",
    "        # 内容：每个张量代表在特定时间步 t 选择的城市索引。也就是说，在解码过程中每一步所选择的城市的索引会被添加到这个列表中。\n",
    "        # 最终结果：当解码完成时，tours 将包含一个长度为 nb_nodes 的列表，每个元素是一个形状为 (bsz,) 的张量，表示批次中每个样本在该时间步选择的城市索引。\n",
    "        tours = []\n",
    "\n",
    "        # list that will contain Float tensors of shape (bsz,) that gives the neg log probs of the choices made at time t\n",
    "        # 类型：这也是一个空列表，但用于保存一系列形状为 (bsz,) 的浮点型张量（FloatTensor）。\n",
    "        # 内容：每个张量代表在特定时间步 t 所做选择的负对数概率（negative log probabilities）。这里使用负对数概率是因为它在优化过程中更方便处理，并且通常用于计算损失函数。\n",
    "        # 最终结果：当解码完成时，sumLogProbOfActions 将包含一个长度为 nb_nodes 的列表，每个元素是一个形状为 (bsz,) 的张量，表示批次中每个样本在该时间步选择动作的负对数概率。\n",
    "        sumLogProbOfActions = []\n",
    "\n",
    "        # key and value for decoder 准备解码器的键和值   \n",
    "        K_att_decoder = self.WK_att_decoder(h_encoder) # size(K_att)=(bsz, nb_nodes+1, dim_emb*nb_layers_decoder)\n",
    "        V_att_decoder = self.WV_att_decoder(h_encoder) # size(V_att)=(bsz, nb_nodes+1, dim_emb*nb_layers_decoder)\n",
    "        \n",
    "        # input placeholder that starts the decoding\n",
    "        # 将位置编码张量移动到与输入张量 x 相同的设备上（例如，CPU 或 GPU）。在 PyTorch 中，确保所有参与计算的张量位于同一设备上是非常重要的，以避免运行时错误并确保高效的计算。\n",
    "        self.PE = self.PE.to(x.device)\n",
    "        idx_start_placeholder = torch.Tensor([nb_nodes]).long().repeat(bsz).to(x.device)\n",
    "\n",
    "        #设置解码起始占位符 h_start，它由编码后的最后一个元素加上位置编码的第一项构成。这是因为在拼接时，起始占位符被添加到了末尾。\n",
    "        #因为占位符在末尾，所以位置编码第一项也放末尾\n",
    "        h_start = h_encoder[zero_to_bsz, idx_start_placeholder, :] + self.PE[0].repeat(bsz,1) # size(h_start)=(bsz, dim_emb)\n",
    "        \n",
    "        # initialize mask of visited cities\n",
    "        # 初始化已访问节点的掩码 mask_visited_nodes，将起始占位符标记为已访问\n",
    "        # torch.zeros(...)：创建一个形状为 (bsz, nb_nodes+1) 的全零张量，其中：\n",
    "        # bsz 是批次大小，表示一批数据中有多少个独立的 TSP 实例。\n",
    "        # nb_nodes 是每个 TSP 实例中的城市数量。\n",
    "        # +1 表示额外添加了一个占位符节点（通常是解码开始时的虚拟节点），因此总共有 nb_nodes + 1 个节点。\n",
    "        # .bool()：将这个全零张量转换为布尔类型的张量，默认值为 False。这意味着在一开始，所有城市都被标记为未访问。\n",
    "        # device=x.device：确保这个张量被创建在与输入张量 x 相同的设备上（例如 CPU 或 GPU），以保证后续计算可以在同一设备上进行。\n",
    "        mask_visited_nodes = torch.zeros(bsz, nb_nodes+1, device=x.device).bool() # False布尔值，只有true和false\n",
    "        # zero_to_bsz：这是一个从 0 到 bsz-1 的索引数组，用来选择每个批次样本对应的行。\n",
    "        # idx_start_placeholder：这是指定了起始占位符节点的索引。由于我们在编码阶段末尾添加了这个占位符，它的索引是 nb_nodes（即最后一个位置）。这行代码的意思是对于每个批次样本，我们将该批次中对应于起始占位符节点的位置标记为已访问。\n",
    "        # 赋值操作：将 mask_visited_nodes 中指定位置的值设置为 True，表示这些位置的城市已经被访问。具体来说，它将每个批次样本中起始占位符节点的位置标记为已访问。\n",
    "        mask_visited_nodes[zero_to_bsz, idx_start_placeholder] = True\n",
    "        \n",
    "        # clear key and val stored in the decoder清除解码器中存储的自注意力键和值，以便开始新的解码过程\n",
    "        self.decoder.reset_selfatt_keys_values()\n",
    "\n",
    "        # construct tour recursively\n",
    "        # 在循环中，对于每一个时间步 t：\n",
    "        # 计算下一节点的概率分布 prob_next_node。解码器根据当前状态 h_t、编码器提供的键和值 K_att_decoder 和 V_att_decoder 以及已访问节点的掩码 mask_visited_nodes 来预测下一个要访问的城市。\n",
    "        # 根据 deterministic 参数选择下一个城市索引 idx。如果为真，则选取概率最高的城市；否则，按照概率分布进行采样。\n",
    "        # 更新当前访问节点的嵌入表示 h_t，并添加相应的时间步位置编码。这一步骤保证了解码器在处理下一个时间步时能考虑到当前的位置信息。\n",
    "        # 更新路径列表 tours 和动作序列的累积对数概率 sumLogProbOfActions。累积对数概率用于评估生成路径的质量。\n",
    "        # 更新已访问节点的掩码 mask_visited_nodes，将刚刚选择的城市标记为已访问，防止重复访问。\n",
    "        h_t = h_start\n",
    "        for t in range(nb_nodes):\n",
    "            \n",
    "            # compute probability over the next node in the tour\n",
    "            prob_next_node = self.decoder(h_t, K_att_decoder, V_att_decoder, mask_visited_nodes) # size(prob_next_node)=(bsz, nb_nodes+1)\n",
    "            \n",
    "            # choose node with highest probability or sample with Bernouilli \n",
    "            if deterministic:\n",
    "                idx = torch.argmax(prob_next_node, dim=1) # size(query)=(bsz,)\n",
    "            else:\n",
    "                idx = Categorical(prob_next_node).sample() # size(query)=(bsz,)\n",
    "            \n",
    "            # compute logprobs of the action items in the list sumLogProbOfActions   \n",
    "            ProbOfChoices = prob_next_node[zero_to_bsz, idx] \n",
    "            sumLogProbOfActions.append( torch.log(ProbOfChoices) )  # size(query)=(bsz,)\n",
    "\n",
    "            # update embedding of the current visited node\n",
    "            h_t = h_encoder[zero_to_bsz, idx, :] # size(h_start)=(bsz, dim_emb)\n",
    "            h_t = h_t + self.PE[t+1].expand(bsz, self.dim_emb)\n",
    "            \n",
    "            # update tour\n",
    "            tours.append(idx)\n",
    "\n",
    "            # update masks with visited nodes\n",
    "            mask_visited_nodes = mask_visited_nodes.clone()\n",
    "            mask_visited_nodes[zero_to_bsz, idx] = True\n",
    "            \n",
    "        # 将 sumLogProbOfActions 和 tours 转换为适当形状的张量，并返回。最终，tours 包含了每一批次中访问城市的顺序，\n",
    "        # 而 sumLogProbOfActions 则提供了这些路径的累积对数概率，可用于损失计算或其他评估指标。\n",
    "        # logprob_of_choices = sum_t log prob( pi_t | pi_(t-1),...,pi_0 )\n",
    "        sumLogProbOfActions = torch.stack(sumLogProbOfActions,dim=1).sum(dim=1) # size(sumLogProbOfActions)=(bsz,)\n",
    "\n",
    "        # convert the list of nodes into a tensor of shape (bsz,num_cities)\n",
    "        tours = torch.stack(tours,dim=1) # size(col_index)=(bsz, nb_nodes)\n",
    "        \n",
    "        return tours, sumLogProbOfActions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EarlyStopping早停减少训练时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EarlyStopping:\n",
    "#     \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "#     def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             patience (int): How long to wait after last time validation loss improved.\n",
    "#                             Default: 7\n",
    "#             verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "#                             Default: False\n",
    "#             delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "#                             Default: 0\n",
    "#             path (str): Path for the checkpoint to be saved to.\n",
    "#                             Default: 'checkpoint.pt'\n",
    "#         \"\"\"\n",
    "#         self.patience = patience\n",
    "#         self.verbose = verbose\n",
    "#         self.counter = 0\n",
    "#         self.best_score = None\n",
    "#         self.early_stop = False\n",
    "#         self.val_loss_min = np.Inf\n",
    "#         self.delta = delta\n",
    "#         self.path = path\n",
    "\n",
    "#     def __call__(self, val_loss, model):\n",
    "\n",
    "#         score = -val_loss\n",
    "\n",
    "#         if self.best_score is None:\n",
    "#             self.best_score = score\n",
    "#             self.save_checkpoint(val_loss, model)\n",
    "#         elif score < self.best_score + self.delta:\n",
    "#             self.counter += 1\n",
    "#             print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "#             if self.counter >= self.patience:\n",
    "#                 self.early_stop = True\n",
    "#         else:\n",
    "#             self.best_score = score\n",
    "#             self.save_checkpoint(val_loss, model)\n",
    "#             self.counter = 0\n",
    "\n",
    "#     def save_checkpoint(self, val_loss, model):\n",
    "#         '''Saves model when validation loss decrease.'''\n",
    "#         if self.verbose:\n",
    "#             print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "#         torch.save(model.state_dict(), self.path)\n",
    "#         self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'nb_nodes': 20, 'bsz': 512, 'dim_emb': 128, 'dim_ff': 512, 'dim_input_nodes': 2, 'nb_layers_encoder': 6, 'nb_layers_decoder': 2, 'nb_heads': 8, 'nb_epochs': 10000, 'nb_batch_per_epoch': 2500, 'nb_batch_eval': 20, 'gpu_id': '0', 'lr': 0.0001, 'tol': 0.001, 'batchnorm': True, 'max_len_PE': 1000}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    del model_train # remove existing model\n",
    "    del model_baseline # remove existing model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 初始化EarlyStopping对象\n",
    "if args.nb_nodes>=20 : early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "if args.nb_nodes>=50 : early_stopping = EarlyStopping(patience=7, verbose=True)\n",
    "if args.nb_nodes>=100 : early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "\n",
    "# model_train：创建一个用于训练的 TSP 网络实例。\n",
    "# model_baseline：创建一个基线模型，通常用于比较性能或计算强化学习中的优势函数（advantage function）。\n",
    "# 参数来源：所有参数都从 args 对象中获取，这通常是通过命令行参数或配置文件传递的超参数集合。\n",
    "model_train = TSP_net(args.dim_input_nodes, args.dim_emb, args.dim_ff, \n",
    "              args.nb_layers_encoder, args.nb_layers_decoder, args.nb_heads, args.max_len_PE,\n",
    "              batchnorm=args.batchnorm)\n",
    "\n",
    "model_baseline = TSP_net(args.dim_input_nodes, args.dim_emb, args.dim_ff, \n",
    "              args.nb_layers_encoder, args.nb_layers_decoder, args.nb_heads, args.max_len_PE,\n",
    "              batchnorm=args.batchnorm)\n",
    "\n",
    "# uncomment these lines if trained with multiple GPUs\n",
    "# 检查多GPU：使用 torch.cuda.device_count() 检查可用的 GPU 数量。\n",
    "# 启用 DataParallel：如果有多个 GPU 可用，则使用 nn.DataParallel 包装模型，使它们能够在多个 GPU 上并行运行，从而加速训练过程。\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.device_count()>1:\n",
    "    model_train = nn.DataParallel(model_train)\n",
    "    model_baseline = nn.DataParallel(model_baseline)\n",
    "# uncomment these lines if trained with multiple GPUs\n",
    "\n",
    "# 选择优化器：为 model_train 创建 Adam 优化器，这是一种常用的自适应学习率优化算法。\n",
    "# 设置学习率：从 args 中获取学习率 lr 并传递给优化器\n",
    "optimizer = torch.optim.Adam( model_train.parameters() , lr = args.lr ) \n",
    "\n",
    "# 指定设备：将两个模型移动到指定的设备上（如 GPU），这里假设 device 已经被正确设置为 'cuda' 或 'cpu'。\n",
    "# 设置基线模型为评估模式：调用 model_baseline.eval()，告诉模型现在处于评估模式，这样可以禁用 dropout 等训练时特有的行为。\n",
    "model_train = model_train.to(device)\n",
    "model_baseline = model_baseline.to(device)\n",
    "model_baseline.eval()\n",
    "\n",
    "print(args); print('')\n",
    "\n",
    "# Logs\n",
    "# 创建日志目录：如果 logs 目录不存在，则创建它。\n",
    "# 生成时间戳：获取当前时间的时间戳，用于命名日志文件。\n",
    "# 打开日志文件：创建一个新的日志文件，并将其打开为写入模式。\n",
    "# 记录时间和参数：将当前时间和所有的超参数写入日志文件，以便日后参考。\n",
    "os.system(\"mkdir logs\")\n",
    "time_stamp=datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "file_name = 'logs'+'/'+time_stamp + \"-n{}\".format(args.nb_nodes) + \"-gpu{}\".format(args.gpu_id) + \".txt\"\n",
    "file = open(file_name,\"w\",1) \n",
    "file.write(time_stamp+'\\n\\n') \n",
    "for arg in vars(args):\n",
    "    file.write(arg)\n",
    "    hyper_param_val=\"={}\".format(getattr(args, arg))\n",
    "    file.write(hyper_param_val)\n",
    "    file.write('\\n')\n",
    "file.write('\\n\\n') \n",
    "plot_performance_train = []\n",
    "plot_performance_baseline = []\n",
    "all_strings = []\n",
    "epoch_ckpt = 0\n",
    "tot_time_ckpt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# compute tours for baseline\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 53\u001b[0m     tour_baseline, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_baseline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# get the lengths of the tours\u001b[39;00m\n\u001b[0;32m     56\u001b[0m L_train \u001b[38;5;241m=\u001b[39m compute_tour_length(x, tour_train) \u001b[38;5;66;03m# size(L_train)=(bsz)\u001b[39;00m\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 125\u001b[0m, in \u001b[0;36mTSP_net.forward\u001b[1;34m(self, x, deterministic)\u001b[0m\n\u001b[0;32m    121\u001b[0m h_t \u001b[38;5;241m=\u001b[39m h_start\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_nodes):\n\u001b[0;32m    123\u001b[0m     \n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# compute probability over the next node in the tour\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     prob_next_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_att_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_att_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_visited_nodes\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# size(prob_next_node)=(bsz, nb_nodes+1)\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m# choose node with highest probability or sample with Bernouilli \u001b[39;00m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ANACONDA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 42\u001b[0m, in \u001b[0;36mTransformer_decoder_net.forward\u001b[1;34m(self, h_t, K_att, V_att, mask)\u001b[0m\n\u001b[0;32m     40\u001b[0m         bsz \u001b[38;5;241m=\u001b[39m h_t\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     41\u001b[0m         q_final \u001b[38;5;241m=\u001b[39m q_final\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_emb)\n\u001b[1;32m---> 42\u001b[0m         attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmyMHA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_att_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV_att_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m] \n\u001b[0;32m     43\u001b[0m prob_next_node \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#attn_weights 被挤压成形状 (bsz, nb_nodes+1) 的张量，表示下一个节点的概率分布 prob_next_node。\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prob_next_node\n",
      "Cell \u001b[1;32mIn[21], line 49\u001b[0m, in \u001b[0;36mmyMHA\u001b[1;34m(Q, K, V, nb_heads, mask, clip_value)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m#attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), float('-inf')) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mmasked_fill(mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1e9\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)应用 softmax 函数，将注意力权重转换为概率分布。\u001b[39;00m\n\u001b[0;32m     50\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attn_weights, V) \u001b[38;5;66;03m# size(attn_output)=(bsz*nb_heads, 1, dim_emb//nb_heads)再次使用批矩阵乘法来计算加权和，得到最终的注意力输出。\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nb_heads\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Uncomment these lines to re-start training with saved checkpoint\n",
    "# checkpoint_file = \"checkpoint/checkpoint_21-03-01--17-25-00-n50-gpu0.pkl\"\n",
    "# checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "# epoch_ckpt = checkpoint['epoch'] + 1\n",
    "# tot_time_ckpt = checkpoint['tot_time']\n",
    "# plot_performance_train = checkpoint['plot_performance_train']\n",
    "# plot_performance_baseline = checkpoint['plot_performance_baseline']\n",
    "# model_baseline.load_state_dict(checkpoint['model_baseline'])\n",
    "# model_train.load_state_dict(checkpoint['model_train'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# print('Re-start training with saved checkpoint file={:s}\\n  Checkpoint at epoch= {:d} and time={:.3f}min\\n'.format(checkpoint_file,epoch_ckpt-1,tot_time_ckpt/60))\n",
    "# del checkpoint\n",
    "# # Uncomment these lines to re-start training with saved checkpoint\n",
    "\n",
    "\n",
    "###################\n",
    "# Main training loop \n",
    "###################\n",
    "#记录开始训练的时间戳，用于后续计算总的训练时间。\n",
    "start_training_time = time.time()\n",
    "\n",
    "# epoch 循环：遍历指定数量的训练周期（args.nb_epochs），每个周期代表一次完整的数据集遍历。\n",
    "# 检查点恢复：如果之前有保存的检查点，则从 epoch_ckpt 开始继续训练。\n",
    "for epoch in range(0,args.nb_epochs):\n",
    "    \n",
    "    # re-start training with saved checkpoint\n",
    "    epoch += epoch_ckpt\n",
    "\n",
    "    ###################\n",
    "    # Train model for one epoch\n",
    "    ###################\n",
    "    # start：记录当前 epoch 开始的时间。\n",
    "    # model_train.train()：将 model_train 设置为训练模式，启用如 dropout 等训练特有的行为。\n",
    "    start = time.time()\n",
    "    model_train.train() \n",
    "\n",
    "    # 生成批次数据：使用 torch.rand 生成一批随机的 TSP 实例，形状为 (bsz, nb_nodes, dim_input_nodes)。\n",
    "    # 计算路径：\n",
    "    # 对于 model_train，以非确定性方式（即基于采样）生成路径，并计算累积对数概率 sumLogProbOfActions。\n",
    "    # 对于 model_baseline，以确定性方式（即选择最高概率的城市）生成路径。\n",
    "    # 计算路径长度：调用 compute_tour_length 函数计算每条路径的总长度。\n",
    "    # 损失计算与反向传播：定义损失函数为 (L_train - L_baseline) * sumLogProbOfActions 的平均值，然后执行反向传播和参数更新。\n",
    "    for step in range(1,args.nb_batch_per_epoch+1):    \n",
    "\n",
    "        # generate a batch of random TSP instances    \n",
    "        x = torch.rand(args.bsz, args.nb_nodes, args.dim_input_nodes, device=device) # size(x)=(bsz, nb_nodes, dim_input_nodes) \n",
    "\n",
    "        # compute tours for model\n",
    "        tour_train, sumLogProbOfActions = model_train(x, deterministic=False) # size(tour_train)=(bsz, nb_nodes), size(sumLogProbOfActions)=(bsz)\n",
    "      \n",
    "        # compute tours for baseline\n",
    "        with torch.no_grad():\n",
    "            tour_baseline, _ = model_baseline(x, deterministic=True)\n",
    "\n",
    "        # get the lengths of the tours\n",
    "        L_train = compute_tour_length(x, tour_train) # size(L_train)=(bsz)\n",
    "        L_baseline = compute_tour_length(x, tour_baseline) # size(L_baseline)=(bsz)\n",
    "        \n",
    "        # backprop\n",
    "        loss = torch.mean( (L_train - L_baseline)* sumLogProbOfActions )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # time_one_epoch：记录完成一个 epoch 所需的时间。\n",
    "    # time_tot：记录总的训练时间，包括之前的检查点时间 tot_time_ckpt。\n",
    "    time_one_epoch = time.time()-start\n",
    "    time_tot = time.time()-start_training_time + tot_time_ckpt\n",
    "\n",
    "        \n",
    "    ###################\n",
    "    # Evaluate train model and baseline on 10k random TSP instances\n",
    "    ###################\n",
    "    #将 model_train 设置为评估模式，禁用训练特有的行为。\n",
    "    model_train.eval()\n",
    "    mean_tour_length_train = 0\n",
    "    mean_tour_length_baseline = 0\n",
    "    #在一定数量的随机 TSP 实例上评估 model_train 和 model_baseline 的平均路径长度。\n",
    "    for step in range(0,args.nb_batch_eval):\n",
    "\n",
    "        # generate a batch of random tsp instances   \n",
    "        x = torch.rand(args.bsz, args.nb_nodes, args.dim_input_nodes, device=device) \n",
    "\n",
    "        # compute tour for model and baseline\n",
    "        with torch.no_grad():\n",
    "            tour_train, _ = model_train(x, deterministic=True)\n",
    "            tour_baseline, _ = model_baseline(x, deterministic=True)\n",
    "            \n",
    "        # get the lengths of the tours\n",
    "        L_train = compute_tour_length(x, tour_train)\n",
    "        L_baseline = compute_tour_length(x, tour_baseline)\n",
    "\n",
    "        # L_tr and L_bl are tensors of shape (bsz,). Compute the mean tour length\n",
    "        mean_tour_length_train += L_train.mean().item()\n",
    "        mean_tour_length_baseline += L_baseline.mean().item()\n",
    "    # 计算平均路径长度：通过累加并除以评估批次的数量来计算平均路径长度。\n",
    "    mean_tour_length_train =  mean_tour_length_train/ args.nb_batch_eval\n",
    "    mean_tour_length_baseline =  mean_tour_length_baseline/ args.nb_batch_eval\n",
    "\n",
    "    # evaluate train model and baseline and update if train model is better\n",
    "    # 条件判断：如果 model_train 的平均路径长度加上一个小容差 args.tol 小于 model_baseline 的平均路径长度，则更新基线模型。\n",
    "    # 更新基线：将 model_train 的状态复制给 model_baseline，确保基线模型总是最优的。\n",
    "    update_baseline = mean_tour_length_train+args.tol < mean_tour_length_baseline\n",
    "    if update_baseline:\n",
    "        model_baseline.load_state_dict( model_train.state_dict() )\n",
    "\n",
    "    # Compute TSPs for small test set\n",
    "    # Note : this can be removed\n",
    "    # 在固定的测试集 x_1000tsp 上评估 model_baseline 的性能，计算平均路径长度。\n",
    "    with torch.no_grad():\n",
    "        tour_baseline, _ = model_baseline(x_1000tsp, deterministic=True)\n",
    "    mean_tour_length_test = compute_tour_length(x_1000tsp, tour_baseline).mean().item()\n",
    "    \n",
    "    # For checkpoint\n",
    "    #性能记录：将每个 epoch 的训练和基线模型的平均路径长度记录到列表中，方便后续绘图或分析。\n",
    "    # 计算优化差距：根据节点数量计算训练模型相对于已知最优解的优化差距。\n",
    "    # 日志输出：打印并写入日志文件，记录每个 epoch 的关键信息，如时间消耗、路径长度等。\n",
    "    plot_performance_train.append([ (epoch+1), mean_tour_length_train])\n",
    "    plot_performance_baseline.append([ (epoch+1), mean_tour_length_baseline])\n",
    "        \n",
    "    # Compute optimality gap\n",
    "    if args.nb_nodes==50: gap_train = mean_tour_length_train/5.692- 1.0\n",
    "    elif args.nb_nodes==100: gap_train = mean_tour_length_train/7.765- 1.0\n",
    "    else: gap_train = -1.0\n",
    "    \n",
    "    # Print and save in txt file\n",
    "    mystring_min = 'Epoch: {:d}, epoch time: {:.3f}min, tot time: {:.3f}day, L_train: {:.3f}, L_base: {:.3f}, L_test: {:.3f}, gap_train(%): {:.3f}, update: {}'.format(\n",
    "        epoch, time_one_epoch/60, time_tot/86400, mean_tour_length_train, mean_tour_length_baseline, mean_tour_length_test, 100*gap_train, update_baseline) \n",
    "    print(mystring_min) # Comment if plot display\n",
    "    file.write(mystring_min+'\\n')\n",
    "#     all_strings.append(mystring_min) # Uncomment if plot display\n",
    "#     for string in all_strings: \n",
    "#         print(string)\n",
    "    \n",
    "    # Saving checkpoint\n",
    "    # 创建检查点目录：确保存在 checkpoint 目录，如果没有则创建。\n",
    "    # 保存检查点：使用 torch.save 将当前训练状态保存到文件中，包括 epoch 数、时间信息、损失值、模型状态字典、优化器状态等，以便后续恢复训练或评估。\n",
    "    checkpoint_dir = os.path.join(\"checkpoint\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'time': time_one_epoch,\n",
    "        'tot_time': time_tot,\n",
    "        'loss': loss.item(),\n",
    "        'TSP_length': [torch.mean(L_train).item(), torch.mean(L_baseline).item(), mean_tour_length_test],\n",
    "        'plot_performance_train': plot_performance_train,\n",
    "        'plot_performance_baseline': plot_performance_baseline,\n",
    "        'mean_tour_length_test': mean_tour_length_test,\n",
    "        'model_baseline': model_baseline.state_dict(),\n",
    "        'model_train': model_train.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        }, '{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + time_stamp + \"-n{}\".format(args.nb_nodes) + \"-gpu{}\".format(args.gpu_id)))\n",
    "    \n",
    "    # early_stopping(mean_tour_length_test, model_train)\n",
    "\n",
    "    # if early_stopping.early_stop:\n",
    "    #     print(\"Early stopping\")\n",
    "    #     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
